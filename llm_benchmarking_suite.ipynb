{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a961fbfb-924d-4f17-8c54-3d4d8bc393f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "torch.cuda.set_per_process_memory_fraction(1.0, device=0)  # 75% of total memory (30GB)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False  # Avoids excessive memory allocation\n",
    "torch.backends.cudnn.enabled = False  # Disables unnecessary optimizations that consume memory\n",
    "torch.cuda.empty_cache()  # Frees up any unused reserved memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()  # Clears all Python garbage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7248b6ca-3484-4f86-9a95-f46b5b9306fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706d481ccaf842c294b2d1c03288b258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, cohen_kappa_score\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29082179-8134-42dd-95a6-16624c15accb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>note</th>\n",
       "      <th>NCTID</th>\n",
       "      <th>trial_title</th>\n",
       "      <th>criterion_type</th>\n",
       "      <th>criterion_text</th>\n",
       "      <th>gpt4_explanation</th>\n",
       "      <th>explanation_correctness</th>\n",
       "      <th>gpt4_sentences</th>\n",
       "      <th>expert_sentences</th>\n",
       "      <th>gpt4_eligibility</th>\n",
       "      <th>expert_eligibility</th>\n",
       "      <th>training</th>\n",
       "      <th>brief_title</th>\n",
       "      <th>phase</th>\n",
       "      <th>drugs</th>\n",
       "      <th>diseases</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>brief_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sigir-20141</td>\n",
       "      <td>0. A 58-year-old African-American woman presen...</td>\n",
       "      <td>NCT01397994</td>\n",
       "      <td>Study to Assess Efficacy of Nicorandil+Atenolo...</td>\n",
       "      <td>inclusion</td>\n",
       "      <td>Patients of chronic stable angina with abnorma...</td>\n",
       "      <td>The patient note does not provide direct evide...</td>\n",
       "      <td>Correct</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>not enough information</td>\n",
       "      <td>not enough information</td>\n",
       "      <td>True</td>\n",
       "      <td>Study to Assess Efficacy of Nicorandil+Atenolo...</td>\n",
       "      <td>Phase 4</td>\n",
       "      <td>['Nicorandil', 'Atenolol']</td>\n",
       "      <td>['Chronic Stable Angina']</td>\n",
       "      <td>40.0</td>\n",
       "      <td>This study is to determine the anti-anginal an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sigir-20141</td>\n",
       "      <td>0. A 58-year-old African-American woman presen...</td>\n",
       "      <td>NCT01397994</td>\n",
       "      <td>Study to Assess Efficacy of Nicorandil+Atenolo...</td>\n",
       "      <td>inclusion</td>\n",
       "      <td>Male and female</td>\n",
       "      <td>The patient is identified as a female in the n...</td>\n",
       "      <td>Correct</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>included</td>\n",
       "      <td>included</td>\n",
       "      <td>True</td>\n",
       "      <td>Study to Assess Efficacy of Nicorandil+Atenolo...</td>\n",
       "      <td>Phase 4</td>\n",
       "      <td>['Nicorandil', 'Atenolol']</td>\n",
       "      <td>['Chronic Stable Angina']</td>\n",
       "      <td>40.0</td>\n",
       "      <td>This study is to determine the anti-anginal an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sigir-20141</td>\n",
       "      <td>0. A 58-year-old African-American woman presen...</td>\n",
       "      <td>NCT01397994</td>\n",
       "      <td>Study to Assess Efficacy of Nicorandil+Atenolo...</td>\n",
       "      <td>inclusion</td>\n",
       "      <td>Age 25 to 65 years</td>\n",
       "      <td>The patient is 58 years old, which falls withi...</td>\n",
       "      <td>Correct</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>included</td>\n",
       "      <td>included</td>\n",
       "      <td>True</td>\n",
       "      <td>Study to Assess Efficacy of Nicorandil+Atenolo...</td>\n",
       "      <td>Phase 4</td>\n",
       "      <td>['Nicorandil', 'Atenolol']</td>\n",
       "      <td>['Chronic Stable Angina']</td>\n",
       "      <td>40.0</td>\n",
       "      <td>This study is to determine the anti-anginal an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sigir-20141</td>\n",
       "      <td>0. A 58-year-old African-American woman presen...</td>\n",
       "      <td>NCT01397994</td>\n",
       "      <td>Study to Assess Efficacy of Nicorandil+Atenolo...</td>\n",
       "      <td>inclusion</td>\n",
       "      <td>Patient must understand and be willing, able a...</td>\n",
       "      <td>The patient note mentions that the patient wil...</td>\n",
       "      <td>Correct</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>included</td>\n",
       "      <td>included</td>\n",
       "      <td>True</td>\n",
       "      <td>Study to Assess Efficacy of Nicorandil+Atenolo...</td>\n",
       "      <td>Phase 4</td>\n",
       "      <td>['Nicorandil', 'Atenolol']</td>\n",
       "      <td>['Chronic Stable Angina']</td>\n",
       "      <td>40.0</td>\n",
       "      <td>This study is to determine the anti-anginal an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sigir-20141</td>\n",
       "      <td>0. A 58-year-old African-American woman presen...</td>\n",
       "      <td>NCT01397994</td>\n",
       "      <td>Study to Assess Efficacy of Nicorandil+Atenolo...</td>\n",
       "      <td>inclusion</td>\n",
       "      <td>Patient must be able to give voluntary written...</td>\n",
       "      <td>The patient note mentions that the patient wil...</td>\n",
       "      <td>Correct</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>included</td>\n",
       "      <td>included</td>\n",
       "      <td>True</td>\n",
       "      <td>Study to Assess Efficacy of Nicorandil+Atenolo...</td>\n",
       "      <td>Phase 4</td>\n",
       "      <td>['Nicorandil', 'Atenolol']</td>\n",
       "      <td>['Chronic Stable Angina']</td>\n",
       "      <td>40.0</td>\n",
       "      <td>This study is to determine the anti-anginal an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   annotation_id   patient_id  \\\n",
       "0              0  sigir-20141   \n",
       "1              1  sigir-20141   \n",
       "2              2  sigir-20141   \n",
       "3              3  sigir-20141   \n",
       "4              4  sigir-20141   \n",
       "\n",
       "                                                note        NCTID  \\\n",
       "0  0. A 58-year-old African-American woman presen...  NCT01397994   \n",
       "1  0. A 58-year-old African-American woman presen...  NCT01397994   \n",
       "2  0. A 58-year-old African-American woman presen...  NCT01397994   \n",
       "3  0. A 58-year-old African-American woman presen...  NCT01397994   \n",
       "4  0. A 58-year-old African-American woman presen...  NCT01397994   \n",
       "\n",
       "                                         trial_title criterion_type  \\\n",
       "0  Study to Assess Efficacy of Nicorandil+Atenolo...      inclusion   \n",
       "1  Study to Assess Efficacy of Nicorandil+Atenolo...      inclusion   \n",
       "2  Study to Assess Efficacy of Nicorandil+Atenolo...      inclusion   \n",
       "3  Study to Assess Efficacy of Nicorandil+Atenolo...      inclusion   \n",
       "4  Study to Assess Efficacy of Nicorandil+Atenolo...      inclusion   \n",
       "\n",
       "                                      criterion_text  \\\n",
       "0  Patients of chronic stable angina with abnorma...   \n",
       "1                                    Male and female   \n",
       "2                                 Age 25 to 65 years   \n",
       "3  Patient must understand and be willing, able a...   \n",
       "4  Patient must be able to give voluntary written...   \n",
       "\n",
       "                                    gpt4_explanation explanation_correctness  \\\n",
       "0  The patient note does not provide direct evide...                 Correct   \n",
       "1  The patient is identified as a female in the n...                 Correct   \n",
       "2  The patient is 58 years old, which falls withi...                 Correct   \n",
       "3  The patient note mentions that the patient wil...                 Correct   \n",
       "4  The patient note mentions that the patient wil...                 Correct   \n",
       "\n",
       "  gpt4_sentences expert_sentences        gpt4_eligibility  \\\n",
       "0      [0, 1, 2]        [0, 1, 2]  not enough information   \n",
       "1            [0]              [0]                included   \n",
       "2            [0]              [0]                included   \n",
       "3            [8]              [8]                included   \n",
       "4            [8]              [8]                included   \n",
       "\n",
       "       expert_eligibility  training  \\\n",
       "0  not enough information      True   \n",
       "1                included      True   \n",
       "2                included      True   \n",
       "3                included      True   \n",
       "4                included      True   \n",
       "\n",
       "                                         brief_title    phase  \\\n",
       "0  Study to Assess Efficacy of Nicorandil+Atenolo...  Phase 4   \n",
       "1  Study to Assess Efficacy of Nicorandil+Atenolo...  Phase 4   \n",
       "2  Study to Assess Efficacy of Nicorandil+Atenolo...  Phase 4   \n",
       "3  Study to Assess Efficacy of Nicorandil+Atenolo...  Phase 4   \n",
       "4  Study to Assess Efficacy of Nicorandil+Atenolo...  Phase 4   \n",
       "\n",
       "                        drugs                   diseases  enrollment  \\\n",
       "0  ['Nicorandil', 'Atenolol']  ['Chronic Stable Angina']        40.0   \n",
       "1  ['Nicorandil', 'Atenolol']  ['Chronic Stable Angina']        40.0   \n",
       "2  ['Nicorandil', 'Atenolol']  ['Chronic Stable Angina']        40.0   \n",
       "3  ['Nicorandil', 'Atenolol']  ['Chronic Stable Angina']        40.0   \n",
       "4  ['Nicorandil', 'Atenolol']  ['Chronic Stable Angina']        40.0   \n",
       "\n",
       "                                       brief_summary  \n",
       "0  This study is to determine the anti-anginal an...  \n",
       "1  This study is to determine the anti-anginal an...  \n",
       "2  This study is to determine the anti-anginal an...  \n",
       "3  This study is to determine the anti-anginal an...  \n",
       "4  This study is to determine the anti-anginal an...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "ground_truth_df = pd.read_csv('Dataset/Final_HF_Data_With_Trial_Info.csv')\n",
    "ground_truth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d1cb2-f58f-4662-a86f-a51a1e785242",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 🚀 **Fix: Efficient GPU Execution (Batch Processing)** ###\n",
    "def generate_response(prompt):\n",
    "    \"\"\"\n",
    "    Generates structured JSON output while ensuring GPU execution.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,  \n",
    "            temperature=0.0,  \n",
    "            max_new_tokens=1024,  \n",
    "            eos_token_id=tokenizer.eos_token_id,  \n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc2758-c980-4f4f-b8ff-d1a8eb12c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 🚀 **Fix: Extract JSON Correctly** ###\n",
    "def extract_json_from_response(response):\n",
    "    \"\"\"\n",
    "    Extracts the first valid JSON object from the model response while ignoring extra text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Locate the JSON block that starts after \"```json\"\n",
    "        json_start = response.find(\"```json\")\n",
    "        if json_start == -1:\n",
    "            return {\"error\": \"No valid JSON block found in response\", \"raw_response\": response}\n",
    "        \n",
    "        # Extract everything after \"```json\"\n",
    "        response = response[json_start + 7:]  # Skip past ```json\n",
    "\n",
    "        # Locate the first valid JSON object in the extracted portion\n",
    "        json_match = re.search(r'\\{.*?\\}', response, re.DOTALL)\n",
    "        if not json_match:\n",
    "            return {\"error\": \"No valid JSON found in response\", \"raw_response\": response}\n",
    "\n",
    "        json_str = json_match.group()  # Extract JSON string\n",
    "\n",
    "        # Try parsing JSON\n",
    "        result = json.loads(json_str)\n",
    "        return result  # Return the extracted JSON\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\"error\": f\"Failed to parse JSON: {str(e)}\", \"raw_json\": json_str}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Unexpected error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeafc47-f528-400a-a255-baf9d5e0b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trial(trial_info: dict, inc_exc: str) -> str:\n",
    "    \"\"\"Given a dict of trial information, returns a string of trial.\"\"\"\n",
    "    \n",
    "    trial = f\"Title: {trial_info.get('trial_title', 'N/A')}\\n\"\n",
    "    \n",
    "    # Ensure diseases and drugs are lists before joining\n",
    "    diseases = trial_info.get(\"diseases\", [])\n",
    "    drugs = trial_info.get(\"drugs\", [])\n",
    "    \n",
    "    if not isinstance(diseases, list):\n",
    "        diseases = [str(diseases)]  # Convert to list if it's a string or None\n",
    "    if not isinstance(drugs, list):\n",
    "        drugs = [str(drugs)]  # Convert to list if it's a string or None\n",
    "    \n",
    "    trial += f\"Target diseases: {', '.join(diseases)}\\n\"\n",
    "    trial += f\"Interventions: {', '.join(drugs)}\\n\"\n",
    "\n",
    "    if inc_exc == \"inclusion\":\n",
    "        trial += f\"Inclusion criteria:\\n {trial_info.get('criterion_text', 'N/A')}\\n\"\n",
    "    elif inc_exc == \"exclusion\":\n",
    "        trial += f\"Exclusion criteria:\\n {trial_info.get('criterion_text', 'N/A')}\\n\"\n",
    "\n",
    "    return trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a9cf4-5245-4f11-be8a-d3e2665fe439",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 🚀 **Fix: Improve Prompt Clarity** ###\n",
    "def get_matching_prompt(trial_info: dict, inc_exc: str, patient: str) -> (str, str):\n",
    "    \"\"\"\n",
    "    Constructs the prompt to ensure only actual criteria are evaluated (not diseases/drugs).\n",
    "    \"\"\"\n",
    "    prompt = f\"You are a clinical trial eligibility assistant. Your task is to compare a given patient note with the {inc_exc} criteria of a clinical trial and determine the patient's eligibility at the criterion level.\\n\\n\"\n",
    "    prompt += \"**IMPORTANT:** Only ONE eligibility criterion is provided at a time.\\n\"\n",
    "    prompt += \"DO NOT evaluate trial metadata such as 'diseases' or 'drugs' as criteria. Only evaluate the provided eligibility criterion.\\n\\n\"\n",
    "    if inc_exc == \"inclusion\":\n",
    "        prompt += (\"Inclusion criteria are the factors that allow someone to participate in a clinical study. \"\n",
    "                   \"They may include characteristics such as age, gender, disease stage, treatment history, and other medical conditions.\\n\\n\")\n",
    "    elif inc_exc == \"exclusion\":\n",
    "        prompt += (\"Exclusion criteria are the factors that disqualify someone from participating in a clinical study. \"\n",
    "                   \"They may include characteristics such as age, gender, disease stage, treatment history, and other medical conditions.\\n\\n\")\n",
    "    if inc_exc == \"inclusion\":\n",
    "        prompt += 'the eligibility_label label must be chosen from {\"not applicable\", \"not enough information\", \"included\", \"not included\"}. \"not applicable\" should only be used for criteria that are not applicable to the patient. \"not enough information\" should be used where the patient note does not contain sufficient information for making the classification. Try to use as less \"not enough information\" as possible because if the note does not mention a medically important fact, you can assume that the fact is not true for the patient. \"included\" denotes that the patient meets the inclusion criterion, while \"not included\" means the reverse.\\n'\n",
    "    elif inc_exc == \"exclusion\":\n",
    "        prompt += 'the eligibility_label label must be chosen from {\"not applicable\", \"not enough information\", \"excluded\", \"not excluded\"}. \"not applicable\" should only be used for criteria that are not applicable to the patient. \"not enough information\" should be used where the patient note does not contain sufficient information for making the classification. Try to use as less \"not enough information\" as possible because if the note does not mention a medically important fact, you can assume that the fact is not true for the patient. \"excluded\" denotes that the patient meets the exclusion criterion and should be excluded in the trial, while \"not excluded\" means the reverse.\\n'\n",
    "    \n",
    "    prompt += \"**Note:** The patient has already provided informed consent for participation. Any criteria related to consent should be considered met.\\n\\n\"\n",
    "    prompt += f\"\\nFor each {inc_exc} criterion, do the following:\\n\"\n",
    "    prompt += \"1. Output the exact text of the criterion as 'criterion_text'.\\n\"\n",
    "    prompt += \"2. Set 'criteria_type' as either 'inclusion' or 'exclusion'.\\n\"\n",
    "    prompt += \"3. Provide a 'brief_reasoning' explaining your evaluation process.\\n\"\n",
    "    prompt += \"4. List 'relevant_sentences' as a list of sentence IDs from the patient note supporting your reasoning.\\n\"\n",
    "    prompt += \"5. Assign an 'eligibility_label'.\\n\"\n",
    "    prompt += \"Output only a JSON object. Do not include any extra text.\\n\\n\"\n",
    "    \n",
    "    user_prompt = f\"Patient note:\\n{patient}\\n\\nTrial Information:\\n\"\n",
    "    user_prompt += f\"- NCTID: {trial_info['NCTID']}\\n\"\n",
    "    user_prompt += f\"- Drugs: {trial_info['drugs']}\\n\"\n",
    "    user_prompt += f\"- Diseases: {trial_info['diseases']} (For context only, NOT an eligibility criterion.)\\n\\n\"\n",
    "    user_prompt += f\"Eligibility Criteria to evaluate:\\n{trial_info['criterion_text']}\\n\\n\"\n",
    "    return prompt, user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db508253-638d-4f81-8b6a-26bbc5121ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Running Model for All Rows on GPU (Batched Processing)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔄 Preparing Inputs: 100%|███████████████| 1015/1015 [00:00<00:00, 19975.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ Running Batched Inference on GPU...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📝 Processing Batches: 100%|█████████████████| 15/15 [1:24:47<00:00, 339.18s/it]\n",
      "📝 Extracting JSON: 100%|████████████████| 1015/1015 [00:00<00:00, 61179.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ FINAL RESULTS (BATCHED PROCESSING ON GPU)\n",
      "\n",
      "Matching process completed! Results saved to eligibility_matching_results_unquant.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "### 🚀 **Fix: Query Model & Debugging** ###\n",
    "def query_model(patient_note, trial_info, inc_exc):\n",
    "    \"\"\"\n",
    "    Queries the model and extracts JSON from the response.\n",
    "    \"\"\"\n",
    "    system_prompt, user_prompt = get_matching_prompt(trial_info, inc_exc, patient_note)\n",
    "    full_prompt = system_prompt + \"\\n\\n\" + user_prompt\n",
    "\n",
    "    # print(\"\\n🚀 DEBUG: Querying Model with Prompt:\\n\", full_prompt[:1000])  # Print first 1000 chars of prompt\n",
    "    \n",
    "    try:\n",
    "        response = generate_response(full_prompt)\n",
    "        print(\"\\n📜 RAW MODEL RESPONSE:\\n\", response)  # Debugging print statement\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: Model failed to generate response:\", str(e))\n",
    "        response = \"\"\n",
    "\n",
    "    extracted_json = extract_json_from_response(response)\n",
    "\n",
    "    print(\"\\n✅ EXTRACTED JSON:\\n\", extracted_json)  # Debugging print statement\n",
    "\n",
    "    return extracted_json\n",
    "\n",
    "\n",
    "### 🚀 **Fix: Process Each Patient-Trial Case** ###\n",
    "def process_patient_trial(patient_id, patient_note, trial, inc_exc):\n",
    "    \"\"\"\n",
    "    Runs the trial evaluation for a single patient and criterion.\n",
    "    \"\"\"\n",
    "    NCTID = trial.get(\"NCTID\", \"N/A\")\n",
    "    evaluation_result = query_model(patient_note, trial, inc_exc)\n",
    "\n",
    "    return {\n",
    "        \"patient_id\": patient_id,\n",
    "        \"NCTID\": NCTID,\n",
    "        \"evaluation_result\": evaluation_result,\n",
    "        \"criteria_type\": inc_exc\n",
    "    }\n",
    "    \n",
    "def save_results_to_json(results, filename=\"eligibility_matching_results.json\"):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"Matching process completed! Results saved to {filename}\")\n",
    "\n",
    "\n",
    "### 🚀 **Fix: Parallel Processing (Only if Needed)** ###\n",
    "test_patient_id = \"sigir-20141\"\n",
    "one_patient_df = ground_truth_df\n",
    "if one_patient_df.empty:\n",
    "    print(\"❌ ERROR: No matching patient ID found in dataset!\")\n",
    "else:\n",
    "    print(\"\\n🚀 Running Model for All Rows on GPU (Batched Processing)...\\n\")\n",
    "\n",
    "    results = []\n",
    "    batch_inputs = []\n",
    "    batch_metadata = []\n",
    "\n",
    "    for idx, row in tqdm(one_patient_df.iterrows(), total=len(one_patient_df), desc=\"🔄 Preparing Inputs\"):\n",
    "        patient_id = row[\"patient_id\"]\n",
    "        patient_note = row[\"note\"]\n",
    "        trial = {\n",
    "            \"criterion_text\": row[\"criterion_text\"], \n",
    "            \"NCTID\": row[\"NCTID\"], \n",
    "            \"drugs\": row[\"drugs\"], \n",
    "            \"diseases\": row[\"diseases\"],\n",
    "            \"trial_title\": row[\"trial_title\"]\n",
    "        }\n",
    "        inc_exc = str(row[\"criterion_type\"]).strip().lower()\n",
    "    \n",
    "        system_prompt, user_prompt = get_matching_prompt(trial, inc_exc, patient_note)\n",
    "        full_prompt = system_prompt + \"\\n\\n\" + user_prompt\n",
    "    \n",
    "        batch_inputs.append(full_prompt)\n",
    "        batch_metadata.append((patient_id, trial, inc_exc))\n",
    "\n",
    "\n",
    "    batch_size = 70  # ✅ Define batch size (adjust based on memory)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        num_batches = (len(batch_inputs) + batch_size - 1) // batch_size  # Compute total batches\n",
    "    \n",
    "        print(\"\\n⚡ Running Batched Inference on GPU...\\n\")\n",
    "    \n",
    "        responses = []\n",
    "        for i in tqdm(range(num_batches), desc=\"📝 Processing Batches\"):\n",
    "            batch_start = i * batch_size\n",
    "            batch_end = min((i + 1) * batch_size, len(batch_inputs))\n",
    "    \n",
    "            batch_chunk = batch_inputs[batch_start:batch_end]  # ✅ Process smaller batches\n",
    "            inputs = tokenizer(batch_chunk, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    \n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                max_new_tokens=1024,  # ✅ Lower max tokens to prevent OOM\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "    \n",
    "            responses.extend([tokenizer.decode(output, skip_special_tokens=True).strip() for output in outputs])\n",
    "    \n",
    "    # ✅ Extract JSON from each response\n",
    "    for i, response in tqdm(enumerate(responses), total=len(responses), desc=\"📝 Extracting JSON\"):\n",
    "        extracted_json = extract_json_from_response(response)\n",
    "        results.append({\n",
    "            \"patient_id\": batch_metadata[i][0],\n",
    "            \"NCTID\": batch_metadata[i][1][\"NCTID\"],\n",
    "            \"evaluation_result\": extracted_json,\n",
    "            \"criteria_type\": batch_metadata[i][2]\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n✅ FINAL RESULTS (BATCHED PROCESSING ON GPU)\\n\")\n",
    "    save_results_to_json(results, \"eligibility_matching_results_unquant.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6087e5f-eebd-4b6b-834f-e4cd92ee54ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TSV file generated and saved to: eligibility_matching_results_unquant.tsv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def convert_json_to_csv(json_filename=\"eligibility_matching_results_unquant.json\", \n",
    "                        csv_filename=\"eligibility_matching_results_unquant.tsv\"):\n",
    "    \"\"\"\n",
    "    Convert the JSON results to a TSV file for benchmarking.\n",
    "    Each evaluation (criterion) becomes one row in the TSV.\n",
    "    The output includes fields:\n",
    "      patient_id, NCTID, criteria_type, criterion_text, brief_reasoning, \n",
    "      relevant_sentences, eligibility_label.\n",
    "    \"\"\"\n",
    "    with open(json_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    rows = []\n",
    "    fieldnames = [\"patient_id\", \"NCTID\", \"criterion_type\", \"criterion_text\", \n",
    "                  \"brief_reasoning\", \"relevant_sentences\", \"eligibility_label\"]\n",
    "    \n",
    "    for result in data:\n",
    "        patient_id = result.get(\"patient_id\", \"\")\n",
    "        NCTID = result.get(\"NCTID\", \"\")\n",
    "        criterion_type = result.get(\"criteria_type\", \"\")\n",
    "        eval_result = result.get(\"evaluation_result\", {})\n",
    "\n",
    "        # Handle cases where the evaluation result is an error\n",
    "        if \"error\" in eval_result:\n",
    "            row = {\n",
    "                \"patient_id\": patient_id,\n",
    "                \"NCTID\": NCTID,\n",
    "                \"criterion_type\": criterion_type,\n",
    "                \"criterion_text\": \"ERROR\",\n",
    "                \"brief_reasoning\": eval_result.get(\"error\", \"\"),\n",
    "                \"relevant_sentences\": \"\",\n",
    "                \"eligibility_label\": \"ERROR\"\n",
    "            }\n",
    "            rows.append(row)\n",
    "            continue  # Skip to the next record\n",
    "\n",
    "        # Extract evaluation details\n",
    "        criterion_text = eval_result.get(\"criterion_text\", \"N/A\")\n",
    "        brief_reasoning = eval_result.get(\"brief_reasoning\", \"No reasoning provided\")\n",
    "        relevant_sentences = \", \".join(map(str, eval_result.get(\"relevant_sentences\", [])))\n",
    "        eligibility_label = eval_result.get(\"eligibility_label\", eval_result.get(\"eligibility\", \"unknown\"))\n",
    "\n",
    "        row = {\n",
    "            \"patient_id\": patient_id,\n",
    "            \"NCTID\": NCTID,\n",
    "            \"criterion_type\": criterion_type,\n",
    "            \"criterion_text\": criterion_text,\n",
    "            \"brief_reasoning\": brief_reasoning,\n",
    "            \"relevant_sentences\": relevant_sentences,\n",
    "            \"eligibility_label\": eligibility_label\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    # Write to TSV file\n",
    "    with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as tsvfile:\n",
    "        writer = csv.DictWriter(tsvfile, fieldnames=fieldnames, delimiter=\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writeheader()\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"✅ TSV file generated and saved to: {csv_filename}\")\n",
    "\n",
    "# Run conversion\n",
    "convert_json_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eee90384-5e7d-4092-aa82-0825b6f56c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted key sample:\n",
      "    patient_id        NCTID  \\\n",
      "0  sigir-20141  nct01397994   \n",
      "1  sigir-20141  nct01397994   \n",
      "2  sigir-20141  nct01397994   \n",
      "3  sigir-20141  nct01397994   \n",
      "4  sigir-20141  nct01397994   \n",
      "\n",
      "                                      criterion_text criterion_type  \n",
      "0  inclusion - patients of chronic stable angina ...      inclusion  \n",
      "1                                    male and female      inclusion  \n",
      "2                                 age 25 to 65 years      inclusion  \n",
      "3  patient must understand and be willing, able a...      inclusion  \n",
      "4  patient must be able to give voluntary written...      inclusion  \n",
      "\n",
      "Ground truth key sample:\n",
      "    patient_id        NCTID  \\\n",
      "0  sigir-20141  nct01397994   \n",
      "1  sigir-20141  nct01397994   \n",
      "2  sigir-20141  nct01397994   \n",
      "3  sigir-20141  nct01397994   \n",
      "4  sigir-20141  nct01397994   \n",
      "\n",
      "                                      criterion_text criterion_type  \n",
      "0  patients of chronic stable angina with abnorma...      inclusion  \n",
      "1                                    male and female      inclusion  \n",
      "2                                 age 25 to 65 years      inclusion  \n",
      "3  patient must understand and be willing, able a...      inclusion  \n",
      "4  patient must be able to give voluntary written...      inclusion  \n",
      "Number of matched records: 785\n",
      "Accuracy: 0.6280254777070063\n",
      "Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "              excluded       0.21      0.88      0.34         8\n",
      "              included       0.77      0.84      0.80       131\n",
      "        not applicable       0.12      0.02      0.04        47\n",
      "not enough information       0.45      0.51      0.48       203\n",
      "          not excluded       0.81      0.69      0.74       378\n",
      "          not included       0.23      0.61      0.34        18\n",
      "\n",
      "              accuracy                           0.63       785\n",
      "             macro avg       0.43      0.59      0.46       785\n",
      "          weighted avg       0.65      0.63      0.63       785\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  7   0   0   0   1   0]\n",
      " [  0 110   0  15   0   6]\n",
      " [  2   1   1  17  25   1]\n",
      " [  9  25   0 104  36  29]\n",
      " [ 15   2   7  94 260   0]\n",
      " [  0   5   0   2   0  11]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "pred_df = pd.read_csv(\"eligibility_matching_results_unquant.tsv\", delimiter=\"\\t\", encoding=\"utf-8\")\n",
    "\n",
    "gt_df = ground_truth_df.copy()\n",
    "\n",
    "# For merging, we need to align key columns.\n",
    "# In the predicted DataFrame, we have:\n",
    "#    patient_id, trial_id, criterion_text, criterion_type, eligibility_label\n",
    "# In the ground truth, we use:\n",
    "#    patient_id, trial_id, criterion_text, criterion_type, expert_eligibility\n",
    "\n",
    "# Normalize the merge keys in both DataFrames.\n",
    "def clean_keys(df, keys):\n",
    "    for key in keys:\n",
    "        df[key] = df[key].astype(str).str.lower().str.strip()\n",
    "    return df\n",
    "\n",
    "merge_keys = [\"patient_id\", \"NCTID\", \"criterion_text\", \"criterion_type\"]\n",
    "pred_df = clean_keys(pred_df, merge_keys)\n",
    "gt_df = clean_keys(gt_df, merge_keys)\n",
    "\n",
    "# check a few key samples:\n",
    "print(\"Predicted key sample:\")\n",
    "print(pred_df[merge_keys].head())\n",
    "print(\"\\nGround truth key sample:\")\n",
    "print(gt_df[merge_keys].head())\n",
    "\n",
    "# Merge the two DataFrames on the common keys.\n",
    "merged_df = pd.merge(\n",
    "    pred_df,\n",
    "    gt_df,\n",
    "    on=merge_keys,\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_pred\", \"_truth\")\n",
    ")\n",
    "\n",
    "print(\"Number of matched records:\", len(merged_df))\n",
    "if len(merged_df) == 0:\n",
    "    print(\"No records matched. Verify that the keys match between your predicted data and ground truth.\")\n",
    "else:\n",
    "    # For evaluation, we compare the predicted eligibility_label with the expert_eligibility from the ground truth.\n",
    "    y_pred = merged_df[\"eligibility_label\"]\n",
    "    y_true = merged_df[\"expert_eligibility\"]\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fa24bee-c199-442f-aca5-2ab6fbb6960d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.6280254777070063\n",
      "Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "              excluded       0.21      0.88      0.34         8\n",
      "              included       0.77      0.84      0.80       131\n",
      "        not applicable       0.12      0.02      0.04        47\n",
      "not enough information       0.45      0.51      0.48       203\n",
      "          not excluded       0.81      0.69      0.74       378\n",
      "          not included       0.23      0.61      0.34        18\n",
      "\n",
      "              accuracy                           0.63       785\n",
      "             macro avg       0.43      0.59      0.46       785\n",
      "          weighted avg       0.65      0.63      0.63       785\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  7   0   0   0   1   0]\n",
      " [  0 110   0  15   0   6]\n",
      " [  2   1   1  17  25   1]\n",
      " [  9  25   0 104  36  29]\n",
      " [ 15   2   7  94 260   0]\n",
      " [  0   5   0   2   0  11]]\n",
      "Misclassified rows saved to misclassified_results.tsv\n",
      "Misclassification counts by ground truth vs. predicted:\n",
      "expert_eligibility      eligibility_label     \n",
      "excluded                not excluded               1\n",
      "included                not enough information    15\n",
      "                        not included               6\n",
      "not applicable          excluded                   2\n",
      "                        included                   1\n",
      "                        not enough information    17\n",
      "                        not excluded              25\n",
      "                        not included               1\n",
      "not enough information  excluded                   9\n",
      "                        included                  25\n",
      "                        not excluded              36\n",
      "                        not included              29\n",
      "not excluded            excluded                  15\n",
      "                        included                   2\n",
      "                        not applicable             7\n",
      "                        not enough information    94\n",
      "not included            included                   5\n",
      "                        not enough information     2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Compute overall accuracy.\n",
    "accuracy = accuracy_score(merged_df[\"expert_eligibility\"], merged_df[\"eligibility_label\"])\n",
    "print(\"Overall Accuracy:\", accuracy)\n",
    "\n",
    "# Print detailed classification report.\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(merged_df[\"expert_eligibility\"], merged_df[\"eligibility_label\"]))\n",
    "\n",
    "# Print confusion matrix.\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(merged_df[\"expert_eligibility\"], merged_df[\"eligibility_label\"]))\n",
    "\n",
    "# Identify misclassified rows.\n",
    "misclassified_df = merged_df[merged_df[\"expert_eligibility\"] != merged_df[\"eligibility_label\"]]\n",
    "\n",
    "# Save misclassified rows to a TSV (tab-delimited) file.\n",
    "misclassified_df.to_csv(\"misclassified_results.tsv\", index=False, sep=\"\\t\", encoding=\"utf-8\")\n",
    "print(\"Misclassified rows saved to misclassified_results.tsv\")\n",
    "\n",
    "# Optional: Print misclassification counts by ground truth vs. predicted.\n",
    "print(\"Misclassification counts by ground truth vs. predicted:\")\n",
    "print(misclassified_df.groupby([\"expert_eligibility\", \"eligibility_label\"]).size())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
